{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.noijjg62emaszi6nyurl6jbkm4evbgm7.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import csv\n",
    "from joblib import Parallel, delayed\n",
    "import concurrent.futures\n",
    "import time Pagination_in-pagination__item__1fF3O Pagination_hideOnMobile__TvjnS Pagination_in-pagination__item--disabled__3zi_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(main):\n",
    "    try:\n",
    "        soup = connect(main)\n",
    "        n_pages_low = [_.get_text(strip=True) for _ in soup.find('div', {'class': 'Pagination_in-pagination__list__2gIAz'}).find_all('a')]\n",
    "        n_pages_low = int(n_pages_low[-1])\n",
    "        \n",
    "        try:\n",
    "            n_pages_high = int(soup.find(\"div\", class_ = 'Pagination_in-pagination__list__2gIAz').find_all(\"div\", class_ = \"Pagination_in-pagination__item__1fF3O Pagination_hideOnMobile__TvjnS Pagination_in-pagination__item--disabled__3zi_j\")[-1].get_text(strip=True))\n",
    "        except:\n",
    "            n_pages_high = 0\n",
    "        \n",
    "        \n",
    "        if n_pages_high > n_pages_low:\n",
    "            n_pages = n_pages_high\n",
    "        else:\n",
    "            n_pages = n_pages_low\n",
    "        \n",
    "\n",
    "        print(n_pages)\n",
    "        #max = soup.find_all(\"span\", class_=\"pagination__number\")\n",
    "        \n",
    "        pages = [main]\n",
    "        \n",
    "        for n in range(2,n_pages+1):    \n",
    "            page_num = \"/?pag={}\".format(n)\n",
    "            pages.append(main + page_num)\n",
    "    except:\n",
    "        pages = [main]\n",
    "    \n",
    "    return pages\n",
    "\n",
    "def connect(web_addr):\n",
    "    resp = requests.get(web_addr)\n",
    "    return BeautifulSoup(resp.content, \"html.parser\")\n",
    "    \n",
    "\n",
    "def get_areas(website):\n",
    "\n",
    "    data = connect(website)\n",
    "    areas = []\n",
    "    for ultag in data.find_all('ul', {'class': 'Breadcrumb_nd-stack__17bqj Breadcrumb_nd-stack--compact__2R4Oe'}):\n",
    "        for litag in ultag.find_all('li'):\n",
    "            for i in range(len(litag.text.split(','))):\n",
    "                areas.append(litag.text.split(',')[i])\n",
    "    areas = [x.strip() for x in areas]\n",
    "    urls = []\n",
    "    \n",
    "    for area in areas:\n",
    "        url = website + '/' + area.replace(' ','-').lower()\n",
    "        urls.append(url)\n",
    "    \n",
    "    return areas, urls\n",
    "\n",
    "def get_apartment_links(website):\n",
    "    data = connect(website)\n",
    "    links = []\n",
    "    for link in data.find_all('ul', {'class': 'results_in-realEstateResults__1LVO0 List_nd-list__3RZR7'}):\n",
    "        for litag in link.find_all('li'):\n",
    "            try:\n",
    "                links.append(litag.a.get('href'))\n",
    "            except:\n",
    "                continue\n",
    "    return links\n",
    "\n",
    "def scrape_link(website):\n",
    "    data = connect(website)\n",
    "    info = data.find_all('dl', {'class': 'im-features__list'})\n",
    "    comp_info = pd.DataFrame()\n",
    "    cleaned_id_text = []\n",
    "    cleaned_id__attrb_text = []\n",
    "    for n in range(len(info)):\n",
    "        for i in info[n].find_all('dt'):\n",
    "            cleaned_id_text.append(i.text)\n",
    "        for i in info[n].find_all('dd'):\n",
    "            cleaned_id__attrb_text.append(i.text)\n",
    "\n",
    "    comp_info['Id'] = cleaned_id_text\n",
    "    comp_info['Attribute'] = cleaned_id__attrb_text\n",
    "    comp_info\n",
    "    feature = []\n",
    "    for item in comp_info['Attribute']:\n",
    "        try:\n",
    "            feature.append(clear_df(item))\n",
    "        except:\n",
    "            feature.append(ultra_clear_df(item))\n",
    "\n",
    "    comp_info['Attribute'] = feature\n",
    "    return comp_info['Id'].values, comp_info['Attribute'].values\n",
    "    \n",
    "\n",
    "def remove_duplicates(x):\n",
    "    return list(dict.fromkeys(x))\n",
    "\n",
    "def clear_df(the_list):\n",
    "    the_list = (the_list.split('\\n')[1].split('  '))\n",
    "    the_list = [value for value in the_list if value != ''][0]\n",
    "    return the_list\n",
    "\n",
    "def ultra_clear_df(the_list):\n",
    "    the_list = (the_list.split('\\n\\n')[1].split('  '))\n",
    "    the_list = [value for value in the_list if value != ''][0]\n",
    "    the_list = (the_list.split('\\n')[0])\n",
    "    return the_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Those are district's links \n",
      "\n",
      "['https://www.immobiliare.it/affitto-case/torino/aurora', 'https://www.immobiliare.it/affitto-case/torino/barriera-di-milano', 'https://www.immobiliare.it/affitto-case/torino/rebaudengo', 'https://www.immobiliare.it/affitto-case/torino/barriera-di-lanzo', 'https://www.immobiliare.it/affitto-case/torino/falchera', 'https://www.immobiliare.it/affitto-case/torino/barca', 'https://www.immobiliare.it/affitto-case/torino/bertolla', 'https://www.immobiliare.it/affitto-case/torino/borgo-san-paolo', 'https://www.immobiliare.it/affitto-case/torino/cenisia', 'https://www.immobiliare.it/affitto-case/torino/borgo-vittoria', 'https://www.immobiliare.it/affitto-case/torino/parco-dora', 'https://www.immobiliare.it/affitto-case/torino/campidoglio', 'https://www.immobiliare.it/affitto-case/torino/san-donato', 'https://www.immobiliare.it/affitto-case/torino/cit-turin', 'https://www.immobiliare.it/affitto-case/torino/cavoretto', 'https://www.immobiliare.it/affitto-case/torino/gran-madre', 'https://www.immobiliare.it/affitto-case/torino/centro', 'https://www.immobiliare.it/affitto-case/torino/colle-della-maddalena', 'https://www.immobiliare.it/affitto-case/torino/superga', 'https://www.immobiliare.it/affitto-case/torino/crocetta', 'https://www.immobiliare.it/affitto-case/torino/san-secondo', 'https://www.immobiliare.it/affitto-case/torino/le-vallette', 'https://www.immobiliare.it/affitto-case/torino/lucento', 'https://www.immobiliare.it/affitto-case/torino/madonna-di-campagna', 'https://www.immobiliare.it/affitto-case/torino/lingotto', 'https://www.immobiliare.it/affitto-case/torino/nizza-millefonti', 'https://www.immobiliare.it/affitto-case/torino/madonna-del-pilone', 'https://www.immobiliare.it/affitto-case/torino/sassi', 'https://www.immobiliare.it/affitto-case/torino/mirafiori-sud', 'https://www.immobiliare.it/affitto-case/torino/pozzo-strada', 'https://www.immobiliare.it/affitto-case/torino/parella', 'https://www.immobiliare.it/affitto-case/torino/regio-parco', 'https://www.immobiliare.it/affitto-case/torino/vanchiglia', 'https://www.immobiliare.it/affitto-case/torino/vanchiglietta', 'https://www.immobiliare.it/affitto-case/torino/san-salvario', 'https://www.immobiliare.it/affitto-case/torino/santa-rita', 'https://www.immobiliare.it/affitto-case/torino/mirafiori-nord']\n"
     ]
    }
   ],
   "source": [
    "## Main website link for city\n",
    "## Get all areas inside the city (districts)\n",
    "\n",
    "website = \"https://www.immobiliare.it/affitto-case/torino\"\n",
    "areas, districts = get_areas(website)\n",
    "print(\"Those are district's links \\n\")\n",
    "print(districts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape cycle initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ca7e3247e54dbf88c87cc9c45b44dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "https://www.immobiliare.it/affitto-case/torino/aurora\n",
      "5\n",
      "https://www.immobiliare.it/affitto-case/torino/barriera-di-milano\n",
      "https://www.immobiliare.it/affitto-case/torino/rebaudengo\n",
      "https://www.immobiliare.it/affitto-case/torino/barriera-di-lanzo\n",
      "https://www.immobiliare.it/affitto-case/torino/falchera\n",
      "https://www.immobiliare.it/affitto-case/torino/barca\n",
      "https://www.immobiliare.it/affitto-case/torino/bertolla\n",
      "https://www.immobiliare.it/affitto-case/torino/borgo-san-paolo\n",
      "12\n",
      "https://www.immobiliare.it/affitto-case/torino/cenisia\n",
      "4\n",
      "https://www.immobiliare.it/affitto-case/torino/borgo-vittoria\n",
      "https://www.immobiliare.it/affitto-case/torino/parco-dora\n",
      "5\n",
      "https://www.immobiliare.it/affitto-case/torino/campidoglio\n",
      "8\n",
      "https://www.immobiliare.it/affitto-case/torino/san-donato\n",
      "6\n",
      "https://www.immobiliare.it/affitto-case/torino/cit-turin\n",
      "2\n",
      "https://www.immobiliare.it/affitto-case/torino/cavoretto\n",
      "https://www.immobiliare.it/affitto-case/torino/gran-madre\n",
      "52\n",
      "https://www.immobiliare.it/affitto-case/torino/centro\n",
      "https://www.immobiliare.it/affitto-case/torino/colle-della-maddalena\n",
      "https://www.immobiliare.it/affitto-case/torino/superga\n",
      "22\n",
      "https://www.immobiliare.it/affitto-case/torino/crocetta\n",
      "6\n",
      "https://www.immobiliare.it/affitto-case/torino/san-secondo\n",
      "https://www.immobiliare.it/affitto-case/torino/le-vallette\n",
      "4\n",
      "https://www.immobiliare.it/affitto-case/torino/lucento\n",
      "3\n",
      "https://www.immobiliare.it/affitto-case/torino/madonna-di-campagna\n",
      "8\n",
      "https://www.immobiliare.it/affitto-case/torino/lingotto\n",
      "6\n",
      "https://www.immobiliare.it/affitto-case/torino/nizza-millefonti\n",
      "3\n",
      "https://www.immobiliare.it/affitto-case/torino/madonna-del-pilone\n",
      "https://www.immobiliare.it/affitto-case/torino/sassi\n",
      "4\n",
      "https://www.immobiliare.it/affitto-case/torino/mirafiori-sud\n",
      "10\n",
      "https://www.immobiliare.it/affitto-case/torino/pozzo-strada\n",
      "8\n",
      "https://www.immobiliare.it/affitto-case/torino/parella\n",
      "3\n",
      "https://www.immobiliare.it/affitto-case/torino/regio-parco\n",
      "9\n",
      "https://www.immobiliare.it/affitto-case/torino/vanchiglia\n",
      "3\n",
      "https://www.immobiliare.it/affitto-case/torino/vanchiglietta\n",
      "18\n",
      "https://www.immobiliare.it/affitto-case/torino/san-salvario\n",
      "13\n",
      "https://www.immobiliare.it/affitto-case/torino/santa-rita\n",
      "4\n",
      "https://www.immobiliare.it/affitto-case/torino/mirafiori-nord\n",
      "--- 21.83938431739807 seconds ---\n",
      "788\n"
     ]
    }
   ],
   "source": [
    "## First of all we need to find all announces' links, in order to scrape informations inside them one by one\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "address = []\n",
    "location = []\n",
    "\n",
    "try:\n",
    "    for url in tqdm(districts):\n",
    "        pages = get_pages(url)\n",
    "        for page in pages:\n",
    "            add = get_apartment_links(page)\n",
    "            address.append(add)\n",
    "            for num in range(0,len(add)):\n",
    "                location.append(url.rsplit('/', 1)[-1])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "announces_links = [item for value in address for item in value]\n",
    "\n",
    "print(len(announces_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "104\n",
      "\n",
      "12\n",
      "8\n",
      "6\n",
      "2\n",
      "5\n",
      "52\n",
      "22\n",
      "6\n",
      "4\n",
      "3\n",
      "6\n",
      "8\n",
      "4\n",
      "3\n",
      "3\n",
      "10\n",
      "8\n",
      "3\n",
      "9\n",
      "1318\n",
      "\n",
      "4\n",
      "--- 55.18163251876831 seconds ---\n",
      "5522\n"
     ]
    }
   ],
   "source": [
    "## ADDEDD Thread Pool\n",
    "## First of all we need to find all announces' links, in order to scrape informations inside them one by one\n",
    "\n",
    "def multi(district):\n",
    "    \n",
    "    pages = get_pages(district)\n",
    "    for page in pages:\n",
    "        try:\n",
    "            add = get_apartment_links(page)\n",
    "            address.append(add)\n",
    "            for num in range(0,len(add)):\n",
    "                location.append(district.rsplit('/', 1)[-1])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return address, location\n",
    "\n",
    "address = []\n",
    "location = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for district in districts:\n",
    "        executor.submit(multi, district)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "announces_links = [item for value in address for item in value]\n",
    "\n",
    "print(len(announces_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numerosity of announces:\n",
      "\n",
      "5522\n"
     ]
    }
   ],
   "source": [
    "## Check that what you scraped has a meaning...and save it\n",
    "\n",
    "print(\"The numerosity of announces:\\n\")\n",
    "print(len(announces_links))\n",
    "with open('announces_list.csv', 'w') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow(announces_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proper announce scraping and dataset creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ed763ee888464fab9b49bf79aa4521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5522 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reindexing only valid with uniquely valued Index objects\n",
      "[195]\n",
      "Reindexing only valid with uniquely valued Index objects\n",
      "[195, 697]\n",
      "Reindexing only valid with uniquely valued Index objects\n",
      "[195, 697, 781]\n",
      "Reindexing only valid with uniquely valued Index objects\n",
      "[195, 697, 781, 1714]\n",
      "Reindexing only valid with uniquely valued Index objects\n",
      "[195, 697, 781, 1714, 3558]\n",
      "Reindexing only valid with uniquely valued Index objects\n",
      "[195, 697, 781, 1714, 3558, 4892]\n",
      "--- 1434.7225329875946 seconds ---\n"
     ]
    }
   ],
   "source": [
    "## Now we pass all announces' links do the scrape_link function to obtain apartments' informations \n",
    "\n",
    "df_scrape = pd.DataFrame()\n",
    "to_be_dropped = []\n",
    "counter = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for link in tqdm(list(announces_links)):\n",
    "    counter=counter+1\n",
    "    try:\n",
    "        names, values = scrape_link(link)\n",
    "        temp_df = pd.DataFrame(columns=names)\n",
    "        temp_df.loc[len(temp_df), :] = values[0:len(names)]\n",
    "        df_scrape = df_scrape.append(temp_df, sort=False)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        to_be_dropped.append(counter)\n",
    "        print(to_be_dropped)\n",
    "        continue\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "Reindexing only valid with uniquely valued Index objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\ttp/ipykernel_652/3995700742.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mfuture_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_completed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mdf_scrape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_scrape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mdf_scrape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[0;32m   8959\u001b[0m             \u001b[0mto_concat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8960\u001b[0m         return (\n\u001b[1;32m-> 8961\u001b[1;33m             concat(\n\u001b[0m\u001b[0;32m   8962\u001b[0m                 \u001b[0mto_concat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8963\u001b[0m                 \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    526\u001b[0m                     \u001b[0mobj_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m                         \u001b[0mindexers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3441\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_as_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3442\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_requires_unique_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3444\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_compare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_interval_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
     ]
    }
   ],
   "source": [
    "## ADDEDD Thread Pool\n",
    "## Now we pass all announces' links do the scrape_link function to obtain apartments' informations \n",
    "\n",
    "df_scrape = pd.DataFrame()\n",
    "to_be_dropped = []\n",
    "counter = 0\n",
    "\n",
    "def get_them(link):\n",
    "    names, values = scrape_link(link)\n",
    "    temp_df = pd.DataFrame(columns=names)\n",
    "    temp_df.loc[len(temp_df), :] = values[0:len(names)]\n",
    "    return temp_df\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    future_list = []\n",
    "    for link in announces_links:\n",
    "        future = executor.submit(get_them, link)\n",
    "        future_list.append(future)\n",
    "    for future in concurrent.futures.as_completed(future_list):\n",
    "        try:\n",
    "            df_scrape = df_scrape.append(future.result(), sort=False)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "df_scrape.reset_index(inplace=True)\n",
    "df_scrape.drop(columns = ['index'], inplace=True)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eventually save useful informations odtained during the scrape process\n",
    "\n",
    "pd.DataFrame(location).to_csv('location.csv', sep=';')\n",
    "pd.DataFrame(to_be_dropped).to_csv('to_be_dropped.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eventually drop announces that reported an error during the scraping process\n",
    "\n",
    "to_be_dropped.sort(reverse=True)\n",
    "\n",
    "for index in to_be_dropped:\n",
    "    del location[index-1]\n",
    "for index in to_be_dropped:\n",
    "    del announces_links[index-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5516, 25)\n"
     ]
    }
   ],
   "source": [
    "## Check df size to see if we have truly collected info and save everything\n",
    "\n",
    "print(df_scrape.shape)\n",
    "df_scrape['district'] = location\n",
    "df_scrape['links'] = announces_links\n",
    "df_scrape.columns = map(str.lower, df_scrape.columns)\n",
    "df_scrape.to_csv('dataset.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrape = pd.read_csv('dataset.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrape = df_scrape[['contratto', 'district', 'tipologia', 'superficie', 'locali', 'piano', 'tipo proprietà', 'prezzo', 'spese condominio', 'spese aggiuntive', 'anno di costruzione', 'stato', 'riscaldamento', 'climatizzazione', 'posti auto', 'links']] #'classe energetica'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unfortunately in most cases informations obtained from direct scrape are a litte bit dirty.\n",
    "## I created a function that cleans my dataset. I did it in a really empirical way, \n",
    "## in order to obtain rows that can be cataloged by a specific dtype.\n",
    "\n",
    "def cleanup(df):\n",
    "    price = []\n",
    "    rooms = []\n",
    "    surface = []\n",
    "    bathrooms = []\n",
    "    floor = []\n",
    "    contract = []\n",
    "    tipo = []\n",
    "    condominio = []\n",
    "    heating = []\n",
    "    built_in = []\n",
    "    state = []\n",
    "    riscaldamento = []\n",
    "    cooling = []\n",
    "    energy_class = []\n",
    "    tipologia = []\n",
    "    pr_type = []\n",
    "    arredato = []\n",
    "    \n",
    "    for tipo in df['tipologia']:\n",
    "        try:\n",
    "            tipologia.append(tipo)\n",
    "        except:\n",
    "            tipologia.append(None)\n",
    "    \n",
    "    for superficie in df['superficie']:\n",
    "        try:\n",
    "            if \"m\" in superficie:\n",
    "                #z = superficie.split('|')[0]\n",
    "                s = superficie.replace(\" m²\", \"\")\n",
    "                surface.append(s)\n",
    "        except:\n",
    "            surface.append(None)\n",
    "    \n",
    "    for locali in df['locali']:\n",
    "        try:\n",
    "            rooms.append(locali[0:1])\n",
    "        except:\n",
    "            rooms.append(None)\n",
    "    \n",
    "    for prezzo in df['prezzo']:\n",
    "        try:\n",
    "            price.append(prezzo.replace(\"Affitto \", \"\").replace(\"€ \", \"\").replace(\"/mese\", \"\").replace(\".\",\"\"))\n",
    "        except:\n",
    "            price.append(None)\n",
    "            \n",
    "    for contratto in df['contratto']:\n",
    "        try:\n",
    "            contract.append(contratto.replace(\"\\n \",\"\"))\n",
    "        except:\n",
    "            contract.append(None)\n",
    "    \n",
    "    for piano in df['piano']:\n",
    "        try:\n",
    "            floor.append(piano.split(' ')[0])\n",
    "        except:\n",
    "            floor.append(None)\n",
    "    \n",
    "    for tipologia in df['tipo proprietà']:\n",
    "        try:\n",
    "            pr_type.append(tipologia.split(',')[0])\n",
    "        except:\n",
    "            pr_type.append(None)\n",
    "            \n",
    "    for condo in df['spese condominio']:\n",
    "        try:\n",
    "            if \"mese\" in condo:\n",
    "                condominio.append(condo.replace(\"€ \",\"\").replace(\"/mese\",\"\"))\n",
    "            else:\n",
    "                condominio.append(None)\n",
    "        except:\n",
    "            condominio.append(None)\n",
    "        \n",
    "    for ii in df['spese aggiuntive']:\n",
    "        try:\n",
    "            if \"anno\" in ii:\n",
    "                mese = int(int(ii.replace(\"€ \",\"\").replace(\"/anno\",\"\").replace(\".\",\"\"))/12)\n",
    "                heating.append(mese)\n",
    "            else:\n",
    "                heating.append(None)\n",
    "        except:\n",
    "            heating.append(None)\n",
    "   \n",
    "    for anno_costruzione in df['anno di costruzione']:\n",
    "        try:\n",
    "            built_in.append(anno_costruzione)\n",
    "        except:\n",
    "            built_in.append(None)\n",
    "    \n",
    "    for stato in df['stato']:\n",
    "        try:\n",
    "            stat = stato.replace(\" \",\"\").lower()\n",
    "            state.append(stat)\n",
    "        except:\n",
    "            state.append(None)\n",
    "    \n",
    "    for tipo_riscaldamento in df['riscaldamento']:\n",
    "        try:\n",
    "            if 'Centralizzato' in tipo_riscaldamento:\n",
    "                riscaldamento.append('centralizzato')\n",
    "            elif 'Autonomo' in tipo_riscaldamento:\n",
    "                riscaldamento.append('autonomo')\n",
    "        except:\n",
    "            riscaldamento.append(None)\n",
    "    \n",
    "    for clima in df['climatizzazione']:\n",
    "        try:\n",
    "            cooling.append(clima.lower().split(',')[0])\n",
    "        except:\n",
    "            cooling.append('None')\n",
    "    \n",
    "    final_df = pd.DataFrame(columns=['contract', 'district', 'renting_type', 'surface', 'locals', 'floor', 'property_type', 'price', 'spese condominio', 'other_expences', 'building_year', 'status', 'heating', 'air_conditioning', 'energy_certificate', 'parking_slots'])#, 'Arredato S/N'])\n",
    "    final_df['contract'] = contract\n",
    "    final_df['renting_type'] = tipologia\n",
    "    final_df['surface'] = surface\n",
    "    final_df['locals'] = rooms\n",
    "    final_df['floor'] = floor\n",
    "    final_df['property_type'] = pr_type\n",
    "    final_df['price'] = price\n",
    "    final_df['spese condominio'] = condominio\n",
    "    final_df['heating_expences'] = heating\n",
    "    final_df['building_year'] = built_in\n",
    "    final_df['status'] = state\n",
    "    final_df['heating_system'] = riscaldamento\n",
    "    final_df['air_conditioning'] = cooling\n",
    "    #final_df['classe energetica'] = energy_class\n",
    "    final_df['district'] = df['district'].values\n",
    "    #inal_df['Arredato S/N'] = arredato\n",
    "    final_df['announce_link'] = announces_links\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the cleaned dataset which is the fruit of your work.\n",
    "\n",
    "final = cleanup(df_scrape)\n",
    "final.to_csv('regression_dataset.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "announces_links = pd.read_csv('announces_list.csv').T.index.values"
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1618264804174,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
