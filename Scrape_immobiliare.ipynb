{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import csv\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(main):\n",
    "    try:\n",
    "        soup = connect(main)\n",
    "        n_pages = [_.get_text(strip=True) for _ in soup.find('ul', {'class': 'pagination pagination__number'}).find_all('li')]\n",
    "        #max = soup.find_all(\"span\", class_=\"pagination__number\")\n",
    "        last_page = int(n_pages[-1])\n",
    "        pages = [main]\n",
    "        \n",
    "        for n in range(2,last_page+1):    \n",
    "            page_num = \"/?pag={}\".format(n)\n",
    "            pages.append(main + page_num)\n",
    "    except:\n",
    "        pages = [main]\n",
    "        \n",
    "    return pages\n",
    "\n",
    "def connect(web_addr):\n",
    "    resp = requests.get(web_addr)\n",
    "    return BeautifulSoup(resp.content, \"html.parser\")\n",
    "    \n",
    "\n",
    "def get_areas(website):\n",
    "    data = connect(website)\n",
    "    areas = []\n",
    "    for ultag in data.find_all('ul', {'class': 'breadcrumb-list breadcrumb-list_list breadcrumb-list__related'}):\n",
    "        for litag in ultag.find_all('li'):\n",
    "            for i in range(len(litag.text.split(','))):\n",
    "                areas.append(litag.text.split(',')[i])\n",
    "    areas = [x.strip() for x in areas]\n",
    "    urls = []\n",
    "    \n",
    "    for area in areas:\n",
    "        url = website + '/' + area.replace(' ','-').lower()\n",
    "        urls.append(url)\n",
    "    \n",
    "    return areas, urls\n",
    "\n",
    "def get_apartment_links(website):\n",
    "    data = connect(website)\n",
    "    links = []\n",
    "    for link in data.find_all('ul', {'class': 'annunci-list'}):\n",
    "        for litag in link.find_all('li'):\n",
    "            try:\n",
    "                links.append(litag.a.get('href'))\n",
    "            except:\n",
    "                continue\n",
    "    return links\n",
    "\n",
    "def scrape_link(website):\n",
    "    data = connect(website)\n",
    "    info = data.find_all('dl', {'class': 'im-features__list'})\n",
    "    comp_info = pd.DataFrame()\n",
    "    cleaned_id_text = []\n",
    "    cleaned_id__attrb_text = []\n",
    "    for n in range(len(info)):\n",
    "        for i in info[n].find_all('dt'):\n",
    "            cleaned_id_text.append(i.text)\n",
    "        for i in info[n].find_all('dd'):\n",
    "            cleaned_id__attrb_text.append(i.text)\n",
    "\n",
    "    comp_info['Id'] = cleaned_id_text\n",
    "    comp_info['Attribute'] = cleaned_id__attrb_text\n",
    "    comp_info\n",
    "    feature = []\n",
    "    for item in comp_info['Attribute']:\n",
    "        try:\n",
    "            feature.append(clear_df(item))\n",
    "        except:\n",
    "            feature.append(ultra_clear_df(item))\n",
    "\n",
    "    comp_info['Attribute'] = feature\n",
    "    return comp_info['Id'].values, comp_info['Attribute'].values\n",
    "    \n",
    "\n",
    "def remove_duplicates(x):\n",
    "    return list(dict.fromkeys(x))\n",
    "\n",
    "def clear_df(the_list):\n",
    "    the_list = (the_list.split('\\n')[1].split('  '))\n",
    "    the_list = [value for value in the_list if value != ''][0]\n",
    "    return the_list\n",
    "\n",
    "def ultra_clear_df(the_list):\n",
    "    the_list = (the_list.split('\\n\\n')[1].split('  '))\n",
    "    the_list = [value for value in the_list if value != ''][0]\n",
    "    the_list = (the_list.split('\\n')[0])\n",
    "    return the_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Those are district's links \n",
      "\n",
      "['https://www.immobiliare.it/affitto-case/torino/aurora', 'https://www.immobiliare.it/affitto-case/torino/barriera-di-milano', 'https://www.immobiliare.it/affitto-case/torino/rebaudengo', 'https://www.immobiliare.it/affitto-case/torino/barriera-di-lanzo', 'https://www.immobiliare.it/affitto-case/torino/falchera', 'https://www.immobiliare.it/affitto-case/torino/barca', 'https://www.immobiliare.it/affitto-case/torino/bertolla', 'https://www.immobiliare.it/affitto-case/torino/borgo-san-paolo', 'https://www.immobiliare.it/affitto-case/torino/cenisia', 'https://www.immobiliare.it/affitto-case/torino/borgo-vittoria', 'https://www.immobiliare.it/affitto-case/torino/parco-dora', 'https://www.immobiliare.it/affitto-case/torino/campidoglio', 'https://www.immobiliare.it/affitto-case/torino/san-donato', 'https://www.immobiliare.it/affitto-case/torino/cit-turin', 'https://www.immobiliare.it/affitto-case/torino/cavoretto', 'https://www.immobiliare.it/affitto-case/torino/gran-madre', 'https://www.immobiliare.it/affitto-case/torino/centro', 'https://www.immobiliare.it/affitto-case/torino/colle-della-maddalena', 'https://www.immobiliare.it/affitto-case/torino/superga', 'https://www.immobiliare.it/affitto-case/torino/crocetta', 'https://www.immobiliare.it/affitto-case/torino/san-secondo', 'https://www.immobiliare.it/affitto-case/torino/le-vallette', 'https://www.immobiliare.it/affitto-case/torino/lucento', 'https://www.immobiliare.it/affitto-case/torino/madonna-di-campagna', 'https://www.immobiliare.it/affitto-case/torino/lingotto', 'https://www.immobiliare.it/affitto-case/torino/nizza-millefonti', 'https://www.immobiliare.it/affitto-case/torino/madonna-del-pilone', 'https://www.immobiliare.it/affitto-case/torino/sassi', 'https://www.immobiliare.it/affitto-case/torino/mirafiori-sud', 'https://www.immobiliare.it/affitto-case/torino/pozzo-strada', 'https://www.immobiliare.it/affitto-case/torino/parella', 'https://www.immobiliare.it/affitto-case/torino/regio-parco', 'https://www.immobiliare.it/affitto-case/torino/vanchiglia', 'https://www.immobiliare.it/affitto-case/torino/vanchiglietta', 'https://www.immobiliare.it/affitto-case/torino/san-salvario', 'https://www.immobiliare.it/affitto-case/torino/santa-rita', 'https://www.immobiliare.it/affitto-case/torino/mirafiori-nord']\n"
     ]
    }
   ],
   "source": [
    "## Main website link for city\n",
    "## Get all areas inside the city (districts)\n",
    "\n",
    "website = \"https://www.immobiliare.it/affitto-case/torino\"\n",
    "areas, districts = get_areas(website)\n",
    "print(\"Those are district's links \\n\")\n",
    "print(districts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape cycle initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead5b8692d7b45f49bc943eb4be92a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=37.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## First of all we need to find all announces' links, in order to scrape informations inside them one by one\n",
    "\n",
    "address = []\n",
    "location = []\n",
    "\n",
    "try:\n",
    "    for url in tqdm(districts):\n",
    "        pages = get_pages(url)\n",
    "        for page in pages:\n",
    "            add = get_apartment_links(page)\n",
    "            address.append(add)\n",
    "            for num in range(0,len(add)):\n",
    "                location.append(url.rsplit('/', 1)[-1])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "        \n",
    "announces_links = [item for value in address for item in value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numerosity of announces:\n",
      "\n",
      "5191\n"
     ]
    }
   ],
   "source": [
    "## Check that what you scraped has a meaning...and save it\n",
    "\n",
    "print(\"The numerosity of announces:\\n\")\n",
    "print(len(announces_links))\n",
    "with open('announces_list.csv', 'w') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow(announces_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proper announce scraping and dataset creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39124520ca14f7b9dae724d094576bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5191.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Now we pass all announces' links do the scrape_link function to obtain apartments' informations \n",
    "\n",
    "df_scrape = pd.DataFrame()\n",
    "to_be_dropped = []\n",
    "counter = 0\n",
    "for link in tqdm(list(announces_links)):\n",
    "    counter=counter+1\n",
    "    try:\n",
    "        names, values = scrape_link(link)\n",
    "        temp_df = pd.DataFrame(columns=names)\n",
    "        temp_df.loc[len(temp_df), :] = values[0:len(names)]\n",
    "        df_scrape = df_scrape.append(temp_df, sort=False)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        to_be_dropped.append(counter)\n",
    "        print(to_be_dropped)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eventually save useful informations odtained during the scrape process\n",
    "\n",
    "pd.DataFrame(location).to_csv('location.csv', sep=';')\n",
    "pd.DataFrame(to_be_dropped).to_csv('to_be_dropped.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eventually drop announces that reported an error during the scraping process\n",
    "\n",
    "to_be_dropped.sort(reverse=True)\n",
    "\n",
    "for index in to_be_dropped:\n",
    "    del location[index-1]\n",
    "for index in to_be_dropped:\n",
    "    del announces_links[index-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-12 22:55:18,689 [3460] WARNING  py.warnings:109: [JupyterRequire] <ipython-input-22-b9c4f3c1d7d9>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_scrape['district'] = location\n",
      "\n",
      "2020-11-12 22:55:18,694 [3460] WARNING  py.warnings:109: [JupyterRequire] <ipython-input-22-b9c4f3c1d7d9>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_scrape['links'] = announces_links\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5191, 16)\n"
     ]
    }
   ],
   "source": [
    "## Check df size to see if we have truly collected info and save everything\n",
    "\n",
    "print(df_scrape.shape)\n",
    "df_scrape['district'] = location\n",
    "df_scrape['links'] = announces_links\n",
    "df_scrape.columns = map(str.lower, df_scrape.columns)\n",
    "df_scrape.to_csv('dataset.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrape = pd.read_csv('dataset.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrape = df_scrape[['contratto', 'district', 'tipologia', 'superficie', 'locali', 'piano', 'tipo proprietà', 'prezzo', 'spese condominio', 'spese aggiuntive', 'anno di costruzione', 'stato', 'riscaldamento', 'climatizzazione', 'posti auto', 'links']] #'classe energetica'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unfortunately in most cases informations obtained from direct scrape are a litte bit dirty.\n",
    "## I created a function that cleans my dataset. I did it in a really empirical way, \n",
    "## in order to obtain rows that can be cataloged by a specific dtype.\n",
    "\n",
    "def cleanup(df):\n",
    "    price = []\n",
    "    rooms = []\n",
    "    surface = []\n",
    "    bathrooms = []\n",
    "    floor = []\n",
    "    contract = []\n",
    "    tipo = []\n",
    "    condominio = []\n",
    "    heating = []\n",
    "    built_in = []\n",
    "    state = []\n",
    "    riscaldamento = []\n",
    "    cooling = []\n",
    "    energy_class = []\n",
    "    tipologia = []\n",
    "    pr_type = []\n",
    "    arredato = []\n",
    "    \n",
    "    for tipo in df['tipologia']:\n",
    "        try:\n",
    "            tipologia.append(tipo)\n",
    "        except:\n",
    "            tipologia.append(None)\n",
    "    \n",
    "    for superficie in df['superficie']:\n",
    "        try:\n",
    "            if \"m\" in superficie:\n",
    "                #z = superficie.split('|')[0]\n",
    "                s = superficie.replace(\" m²\", \"\")\n",
    "                surface.append(s)\n",
    "        except:\n",
    "            surface.append(None)\n",
    "    \n",
    "    for locali in df['locali']:\n",
    "        try:\n",
    "            rooms.append(locali[0:1])\n",
    "        except:\n",
    "            rooms.append(None)\n",
    "    \n",
    "    for prezzo in df['prezzo']:\n",
    "        try:\n",
    "            price.append(prezzo.replace(\"Affitto \", \"\").replace(\"€ \", \"\").replace(\"/mese\", \"\").replace(\".\",\"\"))\n",
    "        except:\n",
    "            price.append(None)\n",
    "            \n",
    "    for contratto in df['contratto']:\n",
    "        try:\n",
    "            contract.append(contratto.replace(\"\\n \",\"\"))\n",
    "        except:\n",
    "            contract.append(None)\n",
    "    \n",
    "    for piano in df['piano']:\n",
    "        try:\n",
    "            floor.append(piano.split(' ')[0])\n",
    "        except:\n",
    "            floor.append(None)\n",
    "    \n",
    "    for tipologia in df['tipo proprietà']:\n",
    "        try:\n",
    "            pr_type.append(tipologia.split(',')[0])\n",
    "        except:\n",
    "            pr_type.append(None)\n",
    "            \n",
    "    for condo in df['spese condominio']:\n",
    "        try:\n",
    "            if \"mese\" in condo:\n",
    "                condominio.append(condo.replace(\"€ \",\"\").replace(\"/mese\",\"\"))\n",
    "            else:\n",
    "                condominio.append(None)\n",
    "        except:\n",
    "            condominio.append(None)\n",
    "        \n",
    "    for ii in df['spese aggiuntive']:\n",
    "        try:\n",
    "            if \"anno\" in ii:\n",
    "                mese = int(int(ii.replace(\"€ \",\"\").replace(\"/anno\",\"\").replace(\".\",\"\"))/12)\n",
    "                heating.append(mese)\n",
    "            else:\n",
    "                heating.append(None)\n",
    "        except:\n",
    "            heating.append(None)\n",
    "   \n",
    "    for anno_costruzione in df['anno di costruzione']:\n",
    "        try:\n",
    "            built_in.append(anno_costruzione)\n",
    "        except:\n",
    "            built_in.append(None)\n",
    "    \n",
    "    for stato in df['stato']:\n",
    "        try:\n",
    "            stat = stato.replace(\" \",\"\").lower()\n",
    "            state.append(stat)\n",
    "        except:\n",
    "            state.append(None)\n",
    "    \n",
    "    for tipo_riscaldamento in df['riscaldamento']:\n",
    "        try:\n",
    "            if 'Centralizzato' in tipo_riscaldamento:\n",
    "                riscaldamento.append('centralizzato')\n",
    "            elif 'Autonomo' in tipo_riscaldamento:\n",
    "                riscaldamento.append('autonomo')\n",
    "        except:\n",
    "            riscaldamento.append(None)\n",
    "    \n",
    "    for clima in df['climatizzazione']:\n",
    "        try:\n",
    "            cooling.append(clima.lower().split(',')[0])\n",
    "        except:\n",
    "            cooling.append('None')\n",
    "    \n",
    "    final_df = pd.DataFrame(columns=['contract', 'district', 'renting_type', 'surface', 'locals', 'floor', 'property_type', 'price', 'spese condominio', 'other_expences', 'building_year', 'status', 'heating', 'air_conditioning', 'energy_certificate', 'parking_slots'])#, 'Arredato S/N'])\n",
    "    final_df['contract'] = contract\n",
    "    final_df['renting_type'] = tipologia\n",
    "    final_df['surface'] = surface\n",
    "    final_df['locals'] = rooms\n",
    "    final_df['floor'] = floor\n",
    "    final_df['property_type'] = pr_type\n",
    "    final_df['price'] = price\n",
    "    final_df['spese condominio'] = condominio\n",
    "    final_df['heating_expences'] = heating\n",
    "    final_df['building_year'] = built_in\n",
    "    final_df['status'] = state\n",
    "    final_df['heating_system'] = riscaldamento\n",
    "    final_df['air_conditioning'] = cooling\n",
    "    #final_df['classe energetica'] = energy_class\n",
    "    final_df['district'] = df['district'].values\n",
    "    #inal_df['Arredato S/N'] = arredato\n",
    "    final_df['announce_link'] = announces_links\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the cleaned dataset which is the fruit of your work.\n",
    "\n",
    "final = cleanup(df_scrape)\n",
    "final.to_csv('regression_dataset.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "announces_links = pd.read_csv('announces_list.csv').T.index.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
